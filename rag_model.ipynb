{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBX-ybHREmLO"
   },
   "source": [
    "# What is RAG?\n",
    "\n",
    "RAG stands for Retrieval Augmented Generation.\n",
    "\n",
    "Each step of simple (naive) RAG can be roughly broken down to:\n",
    "\n",
    "* Retrieval - seeking relevant information from a source (or sources) within an embedded database, given a specific query. For example, getting relevant passages of source text from a database given a question.\n",
    "* Augmented - using the relevant retrieved information to modify an input to a generative model (e.g. an LLM).\n",
    "* Generation - generating an output given an context input. For example, in the case of an LLM, generating a passage of text given an input prompt.\n",
    "\n",
    "Besides the simple RAG technique, various other RAG approaches exist, such as Advanced RAG, Speculative RAG, Fusion RAG, and more. For a better understanding, I highly recommend checking out these two articles: https://homayounsrp.medium.com/6-types-of-retrieval-augmented-generation-rag-techniques-you-should-know-b45de9071c79 and https://bhavikjikadara.medium.com/exploring-the-different-types-of-rag-in-ai-c118edf6d73c.\n",
    "\n",
    "So, RAG is very powerful when we need work with specific custom data and prevent all possible hallucinations about our answer. This approach is also much cheaper than attempting to fine-tune an LLM.\n",
    "\n",
    "RAG can be used as mentioned before with custom data for example:\n",
    "* Customer Q&A support chat\n",
    "* Textbook Q&A\n",
    "* Company internal documentation chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SR0QoA99EmFR"
   },
   "source": [
    "# Import necessary packages and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yzw-NC_kSSKV",
    "outputId": "133d8f36-567f-4628-9db3-0b40895ff247"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain-community PyMuPDF faiss-cpu langchain_huggingface -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4xlgrk6MJ10o"
   },
   "outputs": [],
   "source": [
    "import fitz\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_huggingface import (HuggingFaceEmbeddings,\n",
    "                                   ChatHuggingFace,\n",
    "                                   HuggingFaceEndpoint)\n",
    "\n",
    "from google.colab import userdata\n",
    "token = userdata.get('token_huggingface')\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = token\n",
    "# spacy can be used for making chunks by sentence\n",
    "# from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSoV_oKrEl6O"
   },
   "source": [
    "# Loading pdf files and making vector db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSFE5jS9TgLj"
   },
   "source": [
    "We need to create a custom embeddings database for our RAG model. For this purpose, we can use a vector database such as FAISS or Chroma; in my case, I'll be using FAISS. We also require a transformer encoder model, and I've chosen 'all-MiniLM-L6-v2' from HuggingFace for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528,
     "referenced_widgets": [
      "8e5a61cc7c0d4b409c79fe1a0d625760",
      "860bc36e881245ceb26dacd49433d2b1",
      "5ecf9522b63a48008223c15fc71b6768",
      "fef42f4a24f145babe4067abd99f81eb",
      "17c2cbcea3984711ba0b56e1d258da5e",
      "570ae44280744331b5f207992c21d8cd",
      "1f3ec62cd56c453092ea0225fd168655",
      "70e28229dc7e48ad8312167b13646209",
      "ace1b393f5bc4a73b86b38b6bdef8c06",
      "3e2d8b851d434c93b982b008c043b18e",
      "f4fc23f07afe4abcab5b917361185d5e",
      "01f7cc2f46434cc8895d6a7511957b20",
      "676c3ad57bf1450c95dcea33fc29949a",
      "ba3bc95186eb47d8915b0589e9c4127f",
      "26f8596189b8478dbdc95ce45f57d38c",
      "6d02ecde11204641a2730588ca217c62",
      "0947536a886e4afca43e320643fe9203",
      "0ed205e76be145aaa37148d0634c6e80",
      "2eeae180a3334d458f557d7c480eae77",
      "37cf2eca1a424af793bfd5c23748de44",
      "6f03d3477bfc43e0b97a91722e0c7009",
      "f4351036836741df9c2907576470d707",
      "e8a28ac73df2427bba032f6f78d7a056",
      "b59b37e4aec64adb880ccbd4ee91aca0",
      "3f057a46e5e34e91a688aebe8e374c05",
      "a6a99b4c3fbb4cc5b00cb4870519345d",
      "62790f7429fa43fda644927105ae1368",
      "4ee2b49efea64e109e663240a9174122",
      "b888905ef14a4dbfbd6191c50454a3e1",
      "0acca2afd1814332bf94620a764f00e5",
      "34275746de3f4f26a492cfe3e2dbb7bb",
      "c6e40fd4076240b48bec48c4d1c6a3f9",
      "6abe4de4cd7946e38a04a15ed2f9e81a",
      "99edfd08fbe144af8570da1604025fc5",
      "bc5f53e509234353befee5a1f9f62421",
      "e024e0afb7fd48f88d3b23fab0775b97",
      "8a1ccadd1df44d2582d5480bb853f623",
      "11013484161143bc86c02341922c3279",
      "bc01bc344e3f47df84cc863adfed0608",
      "5359c4b968844b28933d438634427135",
      "0dacd5eb34b44c689e9a50218176d1d3",
      "e5c8f5240dc84138bca01b34d253a824",
      "6f0715cd4bc645e78baa17a58e8bf3db",
      "8a7c5506602e4c5786d3713cf7ab1642",
      "843f713a15354cb9a408bbbcb5d7804b",
      "170a5af6be454e45b55bebeae1ca45cc",
      "e198b338febb4cb6a68ce051c9ca8e88",
      "5c3d36e49a68446784c0600115386f4a",
      "9046a270f5e545189f90479ede2d36e2",
      "9bf5a144554b4747957b4f110b9ebb96",
      "3e1e4555304b49dc9fecca5881bd8e9f",
      "747c8776d3f8450282bbe8f515614202",
      "d1f160572b2745bc9a2d6e42d7c8281f",
      "1b16c6db9f8e43da9947d44076336f84",
      "cc1f72f2528d4259b02b80efd5336724",
      "5e8f142d520643d8ad5468cc109ce2b0",
      "5cb83886e88a46128a609bdffad4184d",
      "7a0357a33dfe4369a97814ab6a72c84b",
      "6bdb235c8fdc49759e28e467e7ecadd5",
      "c668b0ba18874f98b0d7bf2192ba0255",
      "2dddfa16afd84139b2d4dd43a75f20ac",
      "548f44b3c5a44d24b28fe9dbf4a4a7ca",
      "056a0c46c3974354b6590d9d52f9c973",
      "15a66da8ef68498a8afef91e3cbbfb30",
      "f3ceec7e754a46de823bbbd158c843fa",
      "76bf329dd1d2476fb51be707a8b5da92",
      "301fe80e024447029600db648bb51fc6",
      "3a7ab5100eb04692809675a8bbc86d7d",
      "270effe0fd674bab85cb3ee5780a9f94",
      "c7d4e662846d4601ae2164132d404d08",
      "493470dd318b4d8eb528f1c0cc879209",
      "23007482dc7f408c8c038cecc5ab69b2",
      "277b0cfc1b0a42fa9fcea062982ea6d5",
      "d187306454004a6680e655fa459cdf7d",
      "463b33eb5d0243f3819df3248370e345",
      "d3a535e80d46489f8931d586f5570a04",
      "815fe95fbdc345d49391bbaf92c489f6",
      "5330bd9c6f704a0c86d9b36ac81e912c",
      "c70a7181e475409b812c9ef6b0f5c508",
      "fdc8a5abb5fc4ea9ab5aed3509d1a9c5",
      "8d97f9d23a3b4a31ad1169dd705cf862",
      "9aadf542e9204be985305f24335a3b3b",
      "f61b0cca35c946358be2ba144937e0eb",
      "3551cbfc2d7a4670a0354c859bd81518",
      "b66f4fbbbe034c87afff5a6b2eb34f9f",
      "cc0a967f83ab4fb283387c7e9efdbd1b",
      "b51151be6e0448678ca8fac27eee1deb",
      "c8d3d344c10340b29c6da10aad3b125f",
      "cf176350fcaf4a9b9a694c99a6735d4b",
      "50486bc42121474280dc08651e6c940e",
      "4258be06fd554a3796cc0c5fb0cbca22",
      "c205225d364149bd8fd3d6dc35838dd5",
      "e797626bc72149bbb27f69bd6eaa2f6f",
      "cbee09faa1b94cc7a3d317ce7d881850",
      "68e1f77a82764a1bb4e2aa338a6202bd",
      "443b1fd5d29f49d0b98e33cb54cdc093",
      "3ea2125758d04a6b8c3aede6b4a0fa7b",
      "7f975220bad14f04b3b276f7813b51c4",
      "5ce4c85d214744d0bac13663053bc7eb",
      "97e1b12756f149d5b48dcbeb3451d9b5",
      "910ce4fea91e4f65bc411efb29b19545",
      "968acf46520643a483977876336d2bf6",
      "d933e6e545e1429781015989fa33c771",
      "483f4a9c4fba4d08ab2fd2ad3146ed0f",
      "52a7d06310894382bc714391f5f1e91e",
      "39a2199d9b2d4abc9f3e55f58d25789d",
      "1d49bfdc8b144bcb86321d019d707595",
      "afe59f9046604eb4a9cf32ca7e0c549d",
      "1acaaedfd42c42188cdf62b58d3311c4",
      "b189d60a0c7b4a1e88725cbfbc23cfb6",
      "b9dd1f6550f24b9090d898cae0a715be",
      "cd2f308d0a8740938d0067384c98be33",
      "773ba538f9d74abeb9d871de4b09af2d",
      "aea7049ae68547f98aa937700be96c79",
      "c6b69ae03e0d4fc7ab9779b9ca3b0949",
      "3df57846d2944ad7a035fed9557ab033",
      "8470a96d5183447ebf6885be7df197f2",
      "480f2df9524c4a719aff4cdc530e85cd",
      "475e18451cfb4c7298ddef53eeb484bc",
      "9dfb05abf9c444a4aaa5ea4e0da494b4",
      "2b892ecf311b41489758f26abf320743"
     ]
    },
    "id": "yU0yCPLZd04w",
    "outputId": "bcc276e0-9d37-4224-dc2d-6436bdef3b1a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5a61cc7c0d4b409c79fe1a0d625760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f7cc2f46434cc8895d6a7511957b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a28ac73df2427bba032f6f78d7a056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99edfd08fbe144af8570da1604025fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843f713a15354cb9a408bbbcb5d7804b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8f142d520643d8ad5468cc109ce2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301fe80e024447029600db648bb51fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5330bd9c6f704a0c86d9b36ac81e912c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf176350fcaf4a9b9a694c99a6735d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e1b12756f149d5b48dcbeb3451d9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9dd1f6550f24b9090d898cae0a715be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AWUD2kNBOhEB",
    "outputId": "4f46777f-eae2-4d5a-c2f3-f0ae8c6a1f04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Aurélien-Géron-Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-Tensorflow_-Concepts-Tools-and-Techniques-to-Build-Intelligent-Systems-O’Reilly-Media-2019.pdf...\n",
      "Sleeping...\n"
     ]
    }
   ],
   "source": [
    "def text_formatter(text: str) -> str:\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
    "    # we can also provide more formatting functions here\n",
    "    return cleaned_text\n",
    "\n",
    "def create_db(root_directory, embedding_model):\n",
    "  if len(os.listdir(root_directory)) == 0:\n",
    "    print('Nothing was added to the source folder!')\n",
    "    return None\n",
    "  else:\n",
    "    prepared_documents = []\n",
    "    for filename in os.listdir(root_directory):\n",
    "      if filename.endswith('.pdf'):\n",
    "        print(f'Reading {filename}...')\n",
    "\n",
    "        file_path = os.path.join(root_directory, filename)\n",
    "\n",
    "        # using fitz pdf reader more robust\n",
    "        pdf_document = fitz.open(file_path)\n",
    "        # document_metadata = {}\n",
    "        # document_metadata['total_pages'] = len(pdf_document)\n",
    "        # document_metadata['title'] = pdf_document.metadata['title']\n",
    "        doc = [Document(page_content=text_formatter(page.get_text())) for page in pdf_document]\n",
    "        # doc = [Document(page_content=text_formatter(page.get_text()),\n",
    "        #                 metadata={**document_metadata, 'page': page_number}\n",
    "        #                 ) for page_number, page in enumerate(pdf_document)]\n",
    "        # langchain pdf reader\n",
    "        # loader = PyPDFLoader(file_path)\n",
    "        # pdf_document = loader.load()\n",
    "\n",
    "        # can use RecursiveCharacterTextSplitter, TokenTextSplitter, MarkdownHeaderTextSplitter\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=64)\n",
    "        splited_documents = text_splitter.split_documents(doc)\n",
    "        prepared_documents.extend(splited_documents)\n",
    "        print('Sleeping...')\n",
    "        time.sleep(10)\n",
    "    db = FAISS.from_documents(splited_documents, embedding_model)\n",
    "    return db\n",
    "\n",
    "\n",
    "destination_file_path = '/db'\n",
    "db = create_db(\"/content_files\", embedding_model)\n",
    "if db is not None:\n",
    "  db.save_local(destination_file_path)\n",
    "else:\n",
    "  print(\"Root file directory is empty!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Mpt-bZBp2iz"
   },
   "source": [
    "# Retrieval part of the RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ha_H0SVerZh_"
   },
   "outputs": [],
   "source": [
    "# making classic retriever method\n",
    "def retriever(query, db=db, k=3):\n",
    "  retriever = db.as_retriever(search_type=\"similarity_score_threshold\",\n",
    "                              search_kwargs={\"score_threshold\": .4, \"k\": k})\n",
    "  return retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94Py9rdlq7Sn",
    "outputId": "934291fd-5e52-4df4-ae4a-60c23326c5fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-4710c2302fb9>:5: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='7a10b208-72b8-43d7-a884-1c88ed29b6b7', metadata={}, page_content='9 “Random Decision Forests,” T. Ho (1995). 10 The BaggingClassifier class remains useful if you want a bag of something other than Decision Trees. 11 There are a few notable exceptions: splitter is absent (forced to \"random\"), presort is absent (forced to False), max_samples is absent (forced to 1.0), and base_estimator is absent (forced to DecisionTreeClassi fier with the provided hyperparameters). Sampling features results in even more predictor diversity, trading a bit more bias for a lower variance. Random Forests As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of building a BaggingClassifier and pass‐ ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees10 (similarly, there is a RandomForestRegressor class for regression tasks). The following code trains a Random Forest classifier with 500 trees (each limited to maximum 16 nodes), using all available CPU cores: from sklearn.ensemble import RandomForestClassifier rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1) rnd_clf.fit(X_train, y_train) y_pred_rf = rnd_clf.predict(X_test) With a few exceptions, a RandomForestClassifier has all the hyperparameters of a DecisionTreeClassifier (to control how trees are grown), plus all the hyperpara‐ meters of a BaggingClassifier to control the ensemble itself.11 The Random Forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node (see Chapter 6), it searches for the best feature among a random subset of features. This results in a greater tree diversity, which (once again) trades a higher bias for a lower variance, generally yielding an overall better model. The following BaggingClassifier is roughly equivalent to the previous RandomForestClassifier: bag_clf = BaggingClassifier(     DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),     n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1) Random Forests  |  199'),\n",
       " Document(id='7f43a197-3c80-425b-9ed0-13c2ebae567c', metadata={}, page_content='12 “Extremely randomized trees,” P. Geurts, D. Ernst, L. Wehenkel (2005). Extra-Trees When you are growing a tree in a Random Forest, at each node only a random subset of the features is considered for splitting (as discussed earlier). It is possible to make trees even more random by also using random thresholds for each feature rather than searching for the best possible thresholds (like regular Decision Trees do). A forest of such extremely random trees is simply called an Extremely Randomized Trees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. You can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra TreesRegressor class has the same API as the RandomForestRegressor class. It is hard to tell in advance whether a RandomForestClassifier will perform better or worse than an ExtraTreesClassifier. Gen‐ erally, the only way to know is to try both and compare them using cross-validation (and tuning the hyperparameters using grid search). Feature Importance Yet another great quality of Random Forests is that they make it easy to measure the  relative importance of each feature. Scikit-Learn measures a feature’s importance by looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest). More precisely, it is a weighted average, where each node’s weight is equal to the number of training samples that are associated with it (see Chapter 6). Scikit-Learn computes this score automatically for each feature after training, then it scales the results so that the sum of all importances is equal to 1. You can access the result using the feature_importances_ variable. For example, the following code trains a RandomForestClassifier on the iris dataset (introduced in Chapter 4) and outputs each feature’s importance. It seems that the most important features are the petal length (44%) and width (42%), while sepal length and width are rather unim‐ portant in comparison (11% and 2%, respectively). 200  |  Chapter 7: Ensemble Learning and Random Forests'),\n",
       " Document(id='15c84004-4511-4d64-8018-7729dd1fd814', metadata={}, page_content='CHAPTER 7 Ensemble Learning and Random Forests With Early Release ebooks, you get books in their earliest form— the author’s raw and unedited content as he or she writes—so you can take advantage of these technologies long before the official release of these titles. The following will be Chapter 7 in the final release of the book. Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert’s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre‐ dictors is called an ensemble; thus, this technique is called Ensemble Learning, and an Ensemble Learning algorithm is called an Ensemble method. For example, you can train a group of Decision Tree classifiers, each on a different random subset of the training set. To make predictions, you just obtain the predic‐ tions of all individual trees, then predict the class that gets the most votes (see the last exercise in Chapter 6). Such an ensemble of Decision Trees is called a Random Forest,  and despite its simplicity, this is one of the most powerful Machine Learning algo‐ rithms available today. Moreover, as we discussed in Chapter 2, you will often use Ensemble methods near the end of a project, once you have already built a few good predictors, to combine them into an even better predictor. In fact, the winning solutions in Machine Learn‐ ing competitions often involve several Ensemble methods (most famously in the Net‐ flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag‐ ging, boosting, stacking, and a few others. We will also explore Random Forests. 191')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'What is Random Forest?'\n",
    "retrieved_text = retriever(query)\n",
    "retrieved_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDeVRo5t8vSs"
   },
   "source": [
    "# Generation part of the RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14oh2FGd83k-"
   },
   "source": [
    "For this part, we need access to an LLM API, or alternatively, a local LLM such as Llama. In my case, I will use the Hugging Face LLM API for 'HuggingFaceH4/zephyr-7b-beta'. If a more powerful model is required, one can consult the LLM leaderboard on Hugging Face (https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iwyK-Xaq5cw"
   },
   "source": [
    "## The simple approach of Generation part of the RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JqP8lrtAA7E2"
   },
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(repo_id='HuggingFaceH4/zephyr-7b-beta',\n",
    "                          task='text-generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fa-NDXAoCJtj"
   },
   "outputs": [],
   "source": [
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tw0M5Ju3COUZ"
   },
   "outputs": [],
   "source": [
    "def ask(query, context):\n",
    "  messages = [\n",
    "      SystemMessage(content='You are an AI assistant that helps people find information based on provided context. **Always answer on English**'),\n",
    "      # HumanMessage(content = query)\n",
    "      HumanMessage(content=f'Answer the next question: {query} based only on the following context information: {context}. If u don\\'t have specific context you must answer that you don\\'t know!')\n",
    "  ]\n",
    "  response = chat_model.invoke(messages)\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsi5ZHZ3DkoT"
   },
   "outputs": [],
   "source": [
    "def ask_rag(query):\n",
    "  context = [context_text.page_content for context_text in retriever(query)]\n",
    "  return ask(query, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTfjSc8FG9t3"
   },
   "outputs": [],
   "source": [
    "query = 'What is Random Forest?'\n",
    "response = ask_rag(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "85bRMug6rKBV",
    "outputId": "581b8356-5a9e-4c89-8d10-7d80cb9ef255"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Random Forest is an ensemble learning algorithm that combines multiple decision trees to make more accurate and reliable predictions than a single decision tree can provide. Each tree is trained on a different random subset of the features and observations (with replacement) in the training set, which introduces variance in the model and helps to reduce overfitting. The final prediction is made by selecting the class that receives the majority vote from all the individual trees. Random Forest also has additional hyperparameters that allow for greater control over the trees' depth and complexity. It is a popular and effective machine learning algorithm that is widely used in various applications such as fraud detection, customer segmentation, and image classification.\\n\\nThe algorithm is typically trained using the bagging (bootstrapped aggregating) method, which involves creating several samples of the training set with replacement and training a decision tree on each sample. The final prediction is made by combining the predictions of all the trees. However, in Random Forest, the bagging process is combined with feature subsetting, such that each tree is trained on a random subset of features. This further reduces correlation between the trees and leads to a more diverse set of votes during the prediction stage.\\n\\nRandom Forest is implemented in popular machine learning libraries such as Scikit-Learn, where it can be trained using the RandomForestClassifier class for classification tasks, and RandomForestRegressor for regression tasks. Hyperparameter tuning is also possible using techniques like grid search.\\n\\nIn addition to the improved predictive performance compared to a single decision tree, Random Forest also provides an important feature called feature importance. By tracking the relative importance of each feature during the tree learning process, the algorithm provides insights into the contribution of each feature to the final prediction. This can be useful in feature selection and domain expertise.\\n\\nOverall, Random Forest is a powerful and versatile machine learning algorithm, widely used in various applications and industries for its accuracy, reliability, and interpretability.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnPFhGGwrT_q"
   },
   "source": [
    "## Conversation memory\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
